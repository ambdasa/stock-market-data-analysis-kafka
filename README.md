# **ğŸ“Œ Real-Time Stock Market Data Pipeline Using Kafka & AWS**  

## **ğŸš€ Overview**  
This project builds a **real-time stock market data pipeline** using **Apache Kafka, AWS S3, AWS Glue, and Amazon Athena** to simulate stock price movements, ingest streaming data, and enable real-time analytics.  

ğŸ”¹ **Kafka Producer** streams simulated stock data.  
ğŸ”¹ **Kafka Broker (EC2)** handles real-time data ingestion.  
ğŸ”¹ **Kafka Consumer** processes data and stores it in **Amazon S3**.  
ğŸ”¹ **AWS Glue Crawler** detects schema and updates the **Glue Data Catalog**.  
ğŸ”¹ **Amazon Athena** enables **SQL queries** on stock market data.  

---

## **ğŸ›  Tech Stack**  
- **Programming Language**: Python (Boto3, Kafka-python, Pandas)  
- **Messaging System**: Apache Kafka (Hosted on AWS EC2)  
- **Cloud Storage**: Amazon S3  
- **ETL & Schema Detection**: AWS Glue & Glue Data Catalog  
- **Query Engine**: Amazon Athena  
- **Infrastructure**: AWS IAM, AWS EC2, AWS Glue, AWS Athena  

---

## **ğŸ“Œ Architecture Overview**  



1ï¸âƒ£ **Kafka Producer** simulates real-time stock prices from a dataset and sends messages to Kafka.  
2ï¸âƒ£ **Kafka Broker (on EC2)** handles real-time message ingestion.  
3ï¸âƒ£ **Kafka Consumer** processes stock data and writes structured data to **Amazon S3**.  
4ï¸âƒ£ **AWS Glue Crawler** scans the raw data and updates the **Glue Data Catalog**.  
5ï¸âƒ£ **Amazon Athena** enables SQL-based analysis directly on S3 data.  

---

## **ğŸ“‚ Project Structure**  
```
â”œâ”€â”€ producer.py             # Kafka producer for simulating stock market data
â”œâ”€â”€ consumer.py             # Kafka consumer for processing data
â”œâ”€â”€ config.py               # Configuration for Kafka, AWS keys, S3 bucket
â”œâ”€â”€ setup_kafka.sh          # Kafka setup script for AWS EC2
â”œâ”€â”€ README.md               # Project documentation
â””â”€â”€ dataset/                # Sample stock market dataset (CSV)
```

---

## **ğŸš€ Setup & Installation**  

### **1ï¸âƒ£ Prerequisites**  
âœ” **AWS Account** (Free Tier)  
âœ” **EC2 Instance (Ubuntu/Amazon Linux)** for hosting Kafka  
âœ” **IAM Role** with S3, Glue, and Athena permissions  
âœ” **Python 3.x** and required dependencies  

### **2ï¸âƒ£ Clone Repository & Install Dependencies**  
```bash
git clone https://github.com/your-repo/kafka-stock-market-pipeline.git
cd kafka-stock-market-pipeline
pip install -r requirements.txt
```

### **3ï¸âƒ£ Set Up Kafka on AWS EC2**  
```bash
chmod +x setup_kafka.sh
./setup_kafka.sh
```

### **4ï¸âƒ£ Start Kafka Broker & Zookeeper**  
```bash
bin/zookeeper-server-start.sh config/zookeeper.properties
bin/kafka-server-start.sh config/server.properties
```

### **5ï¸âƒ£ Run the Kafka Producer & Consumer**  
**Start the producer to send stock market data:**  
```bash
python producer.py
```
**Start the consumer to process & store data in S3:**  
```bash
python consumer.py
```

---

## **ğŸ“Œ AWS Setup**  

### **ğŸ›  Create an S3 Bucket for Storing Data**  
1ï¸âƒ£ Go to **AWS S3** â†’ **Create Bucket** â†’ Name it `kafka-stock-market-data`.  
2ï¸âƒ£ Enable public access for testing (or set IAM policies for secure access).  

### **ğŸ›  Set Up AWS Glue Crawler**  
1ï¸âƒ£ Navigate to **AWS Glue** â†’ **Crawlers** â†’ **Create Crawler**.  
2ï¸âƒ£ Choose S3 as the data source and select `kafka-stock-market-data`.  
3ï¸âƒ£ Set up an IAM role with `AmazonS3FullAccess` & `AWSGlueServiceRole`.  
4ï¸âƒ£ Run the Glue Crawler to create a **Glue Data Catalog Table**.  

### **ğŸ›  Query Data Using Amazon Athena**  
1ï¸âƒ£ Open **Amazon Athena**.  
2ï¸âƒ£ Select the **Glue Catalog database** and run:  
```sql
SELECT * FROM stock_data LIMIT 10;
```

---

## **âš¡ Challenges & Solutions**  
### **ğŸ”¹ Inconsistent Message Processing Due to Network Latency**  
âœ” Adjusted Kafka producer batch size & compression to optimize network usage.  

### **ğŸ”¹ S3 Data Not Reflecting in Athena Queries**  
âœ” Enabled **schema evolution in Glue** & ensured the Glue Crawler runs after new data ingestion.  

### **ğŸ”¹ Consumer Lag in Kafka Processing**  
âœ” Tuned Kafka consumer configurations (`auto.offset.reset = 'earliest'`).  

### **ğŸ”¹ IAM Permission Issues Prevented Glue from Accessing S3**  
âœ” Created a dedicated IAM role with **AmazonS3FullAccess & AWSGlueServiceRole**.  

### **ğŸ”¹ Slow Query Performance in Athena**  
âœ” Implemented **partitioning & compression** for optimized query speed & cost efficiency.  

---

## **ğŸ¯ Final Results**  
âœ… **Real-time data pipeline from Kafka â†’ S3 â†’ Glue â†’ Athena**  
âœ… **Optimized for high-throughput, low-latency analytics**  
âœ… **Scalable, cost-effective solution for real-time financial data analysis**  

---
